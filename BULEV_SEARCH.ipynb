{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/linuxconfig/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import scipy\n",
    "import binascii\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import base64\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import csv\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import gzip\n",
    "import pymorphy2\n",
    "import zlib\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import importlib\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import xgboost\n",
    "import zipfile\n",
    "import gzip\n",
    "import string\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "from io import BytesIO\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import namedtuple\n",
    "from scipy import sparse\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "from bs4 import BeautifulSoup\n",
    "from string import punctuation\n",
    "from bs4.element import Comment\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing.pool import ThreadPool\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis  \n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier  \n",
    "from sklearn.svm import SVC \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier,BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier\n",
    "from zipfile import ZipFile \n",
    "import gzip\n",
    "import shutil\n",
    "import re\n",
    "import operator\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "import ast\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize , word_tokenize\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# небольшой пресказ хода работы\n",
    "# сначала просто в лоб парсим все сайты и получаем индекс который  представляет из себя словарь где ключ это id страницы, а значение это список где 1ый элемент это урл а второй это слова на  странице(уже привеенные в норм форму)\n",
    "# далее я создал лист всех текстов страниц обьединил его и сделалсет(словарь) всех слов которые есть \n",
    "# далее пробежался по этому сету и отскал сколько раз и в каких документах встречается слово (сделал в лоб) может показаться что это тупо - да так и есть но доков не много и все было довольно быстро (заварил чай поситал хабр и готово)\n",
    "# далее идет обработка запроса и выдача \n",
    "# еще говорили что надо использовать сжатие индекса я его реализовал но не внедрил это в модель (внедрить не сложно просто пробегаемся по обратному индексу и шифруем постинг листы, когда надо достать инфу то дешифруем данные и берем)\n",
    "# не иплементировал сжатие так как simple9 хорош когда постинг листы очень длинный но тут постинг листы довольно короткие\n",
    "# если выдача будет не очень точной скорее вссего при приведении в норм форму пайморфи мог немного ошибиться "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = []\n",
    "morph = morph = pymorphy2.MorphAnalyzer()\n",
    "names = ['data_bool/lenta.ru_4deb864d-3c46-45e6-85f4-a7ff7544a3fb_01.gz',\n",
    "         'data_bool/lenta.ru_80e74243-83da-4367-8ae3-fe38d333f283_01.gz',\n",
    "         'data_bool/lenta.ru_159b9f4b-972b-48b1-8ec3-44fbd6be33c4_01.gz',\n",
    "         'data_bool/lenta.ru_6398c7e2-16da-40d2-8923-95f65aaaeb07_01.gz',\n",
    "         'data_bool/lenta.ru_aa5a1ef9-6ca4-4dc7-890f-308d4d62db59_01.gz',\n",
    "         'data_bool/lenta.ru_b81aa623-ba55-43dc-b3c5-47ae2253ad27_01.gz',\n",
    "         'data_bool/lenta.ru_b6838708-1aa9-496f-bf88-e277374f93a8_01.gz',\n",
    "         'data_bool/lenta.ru_d1f7e910-b5f1-4719-b724-090093e143fe_01.gz']\n",
    "for i in names:\n",
    "    file = gzip.open('data_bool/lenta.ru_4deb864d-3c46-45e6-85f4-a7ff7544a3fb_01.gz')\n",
    "    content = file.read()\n",
    "    pages.append(content)\n",
    "separate_pages = []\n",
    "for i in pages:\n",
    "    separate_pages.append(re.split(r'http:',i.decode(\"utf-8\",'ignore')))\n",
    "all_pages_sep = reduce(operator.concat, separate_pages)\n",
    "all_pages_sep = ['http:'+item for item in all_pages_sep if len(item)>101]\n",
    "doc_id = 0\n",
    "index = {}\n",
    "regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "for i in all_pages_sep:\n",
    "    i = i.lower()\n",
    "    url = re.findall(regex,i)[0][0]\n",
    "    words = re.findall(r'\\w+',i)\n",
    "    words = [morph.parse(j)[0].normal_form for j in words]\n",
    "    index[int(doc_id)] = [url,words]\n",
    "    doc_id +=1\n",
    "word_corpus_sep = np.array(list(index.values()))[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('index.pickle', 'wb') as handle:\n",
    "#     pickle.dump(index, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('index.pickle', 'rb') as handle:\n",
    "    index = pickle.load(handle)\n",
    "\n",
    "# print(index == index_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_corpus_sep = np.array(list(index.values()))[:,1:]\n",
    "word_corpus = []\n",
    "for i in word_corpus_sep:\n",
    "    for j in i:\n",
    "        word_corpus.append(j)\n",
    "w = []\n",
    "for i in word_corpus_sep:\n",
    "    for j in i:\n",
    "        for j in j:\n",
    "            w.append(j)\n",
    "words_corpus_set = set(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_index = {}\n",
    "doc_id = 0\n",
    "for i in words_corpus_set:\n",
    "    imbed = []\n",
    "    doc_id = 0\n",
    "    for j in word_corpus:\n",
    "        im = j.count(i)\n",
    "        if im == 0:\n",
    "            doc_id += 1\n",
    "        else:\n",
    "            imbed.append([doc_id,im])\n",
    "        \n",
    "    reverse_index[i] = imbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "with open('reverse_index.pickle', 'wb') as handle:\n",
    "    pickle.dump(reverse_index, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('reverse_index.pickle', 'rb') as handle:\n",
    "    reverse_index_r = pickle.load(handle)\n",
    "\n",
    "print(reverse_index == reverse_index_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_simple = [[28,         1, SIMPLE_9, 1],\n",
    "             [14,         3, SIMPLE_8, 2],\n",
    "             [ 9,         7, SIMPLE_7, 3],\n",
    "             [ 7,        15, SIMPLE_6, 4],\n",
    "             [ 5,        31, SIMPLE_5, 5],\n",
    "             [ 4,       127, SIMPLE_4, 7],\n",
    "             [ 3,       511, SIMPLE_3, 9],\n",
    "             [ 2,     16383, SIMPLE_2,14],\n",
    "             [ 1, 268435455, SIMPLE_1,28]\n",
    "           ]\n",
    "def simple9_encode(a, length):\n",
    "    global en_simple\n",
    "    off = 0        # count encode length\n",
    "    result = []\n",
    "    while off < length:\n",
    "        for t in en_simple:\n",
    "            n, threshold, code, shift = t[0], t[1], t[2], t[3]\n",
    "            if off + n <= length and max(a[off:off + n]) <= t[1]:\n",
    "                tmp = a[off]\n",
    "                for i in range(1, n):\n",
    "                    tmp |= (a[off + i] << (shift * i))\n",
    "                result.append(tmp | t[2])\n",
    "                off += n\n",
    "                break\n",
    "    return result\n",
    "def simple9_decode(a):\n",
    "    de_simple = {SIMPLE_9: [28, 0x1, 1],\n",
    "                 SIMPLE_8: [14, 0x3, 2],\n",
    "                 SIMPLE_7: [9, 0x7, 3],\n",
    "                 SIMPLE_6: [7, 0xf, 4],\n",
    "                 SIMPLE_5: [5, 0x1f, 5],\n",
    "                 SIMPLE_4: [4, 0x7f, 7],\n",
    "                 SIMPLE_3: [3, 0x1ff, 9],\n",
    "                 SIMPLE_2: [2, 0x3fff, 14],\n",
    "                 SIMPLE_1: [1, 0xfffffff, 28]\n",
    "                 }\n",
    "    result = []\n",
    "    for t in a:\n",
    "        code = t & 0xf0000000\n",
    "        data = t & 0xfffffff\n",
    "        info = de_simple[code]\n",
    "        n, bit, shift = info[0], info[1], info[2]\n",
    "        for i in range(n):\n",
    "            result.append(data & bit)\n",
    "            data >>= shift\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolean_query(string, rec_level=0):\n",
    "    first_arg = None\n",
    "    second_arg = None\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    if rec_level == 0 and '&' not in string and '|' not in string:\n",
    "        first_arg = morph.parse(string)[0].normal_form\n",
    "        list1 = np.array(reverse_index[first_arg])[:,0]\n",
    "        return list1\n",
    "    if '(' in string:\n",
    "        possible_arg = string.split('(')[0]\n",
    "        if len(possible_arg) > 2:\n",
    "            first_arg = possible_arg.split('&')[0].split('|')[0]\n",
    "            first_arg = morph.parse(first_arg)[0].normal_form\n",
    "            list1 = np.array(reverse_index[first_arg])[:,0]\n",
    "            if string[len(first_arg)] == '&':\n",
    "                list2 = boolean_query(string[(len(first_arg) + 2):-1], rec_level = rec_level + 1)\n",
    "                total_list = np.sort(np.intersect1d(list1, list2))\n",
    "            else:\n",
    "                list2 = boolean_query(string[(len(first_arg) + 2):-1], rec_level = rec_level + 1)\n",
    "                total_list = np.sort(np.append(list1, list2))\n",
    "            return total_list\n",
    "    else:\n",
    "        first_arg = string.split('&')[0].split('|')[0]\n",
    "        first_arg = morph.parse(first_arg)[0].normal_form\n",
    "        if string[len(first_arg)] == '&':\n",
    "            second_arg = string.split('&')[1]\n",
    "            second_arg = morph.parse(second_arg)[0].normal_form\n",
    "            list1 =np.array(reverse_index[first_arg])[:,0]\n",
    "            list2 = np.array(reverse_index[second_arg])[:,0]\n",
    "            total_list =  np.sort(np.intersect1d(list1, list2))\n",
    "            print(len(total_list))\n",
    "            return total_list\n",
    "        else:\n",
    "            second_arg = string.split('|')[1]\n",
    "            second_arg = morph.parse(second_arg)[0].normal_form\n",
    "            list1 = np.array(reverse_index[first_arg])[:,0]\n",
    "            list2 = np.array(reverse_index[second_arg])[:,0]\n",
    "            total_list = np.sort(np.append(list1, list2))\n",
    "            return total_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_snippet(query):\n",
    "    a =  boolean_query(query)\n",
    "    for i in a:\n",
    "        url = '/'.join(np.array(index[i])[0].split('/')[0:-1])\n",
    "        print(i,url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "пожар&беда\n",
      "1\n",
      "5174 http://lenta.ru/news/2011/02/09/tweet\n"
     ]
    }
   ],
   "source": [
    "query = input()\n",
    "simple_snippet(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
